{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "- **Section 1:** Initially the user is asked whether they want the Chicago dataset or the Las-Vegas dataset. When the user decides which dataset they want. We read in the plain text file. We then parse all the feature subsets from the text file that we will need for the classifier.\n",
    "- **Section 2:** Create a data frame with all the features that we collected from the dataset about each review. Store this data frame in a .csv file.\n",
    "- **Section 3:** Extract the additional feature subset from the dataset and create an additional data frame which has the additional feature subset. Store this data frame in a .csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key - Table\n",
    "\n",
    "- R1: The mean review helpfulness over all reviews authored by the user.\n",
    "- R2: The standard deviation of review helpfulness over all reviews authored by the user.\n",
    "- R3: The percentage of reviews authored by the user which have received a minimum of T opinions; in this work, T = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_decision = int(input('Enter 1 to read Chicago dataset and enter 2 to read Las-Vegas dataset'))\n",
    "if user_decision == 1:\n",
    "    text_file = 'chicago.txt'\n",
    "else:\n",
    "    text_file = 'las-vegas.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(text_file, 'r', encoding='mac_roman')\n",
    "list_of_lines = []\n",
    "for line in f:\n",
    "    list_of_lines.append(line)\n",
    "text_ = ''.join(list_of_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = r'<review>\\n([\\s\\S]*?)\\n</review>'\n",
    "review_count = re.findall(regex, text_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each review is converted to an element of a list then we use the Python pickle library to write this list of reviews directly to a text file which can be read directly as a list from the user without having to parse the file into a list again when they read it in. The file is chosen depending on the users choice of dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if user_decision == 1:\n",
    "    with open(\"reviews_file_chicago.txt\", \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(review_count, fp)\n",
    "else:\n",
    "    with open(\"reviews_file_las-vegas.txt\", \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(review_count, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listToString(s):  \n",
    "    str1 = \" \" \n",
    "    return (str1.join(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the hotel ids from the dataset, creating a list of hotel ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = r'<hotelUrl>\\n([\\s\\S]*?)\\n</hotelUrl>'\n",
    "hotel_id = []\n",
    "for i in review_count:\n",
    "    string = listToString(re.findall(regex, i))\n",
    "    hotel_id.append(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the reviewer ids from the dataset, creating a list of reviewer ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewer_id = []\n",
    "\n",
    "regex = r'<memberUrl>\\n([\\s\\S]*?)\\n</memberUrl>'\n",
    "\n",
    "for i in review_count:\n",
    "    string = listToString(re.findall(regex, i))\n",
    "    reviewer_id.append(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the helpfulness score from the dataset, creating a list in the form \"X of Y\" found this to be helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = r'\\n</memberUrl>[\\s\\S]*?\\n<helpfulness>\\n(.*?)\\n</helpfulness>'\n",
    "helpfulness_scores = []\n",
    "for i in review_count:\n",
    "    helpfulness_scores.append(re.findall(regex, i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the percentage of helpful helpfulness opnions which is the helpfulness score for the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "all_values = []\n",
    "\n",
    "for item in helpfulness_scores:\n",
    "    item = listToString(item)\n",
    "    value = item.replace(' of ', '/')\n",
    "    if value.endswith('/0'):\n",
    "        all_values.append(0)\n",
    "        continue\n",
    "    all_values.append(eval(value))\n",
    "\n",
    "count = 0\n",
    "for i in all_values:\n",
    "    helpfulness_scores[count] = i\n",
    "    count += 1\n",
    "\n",
    "average_helpfulness_score = mean(all_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avoid_zero_division(d):\n",
    "    return d==0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dictionary with the R1 values to add to the data frame later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = r'<memberUrl>\\n(.*?)\\n</memberUrl>[\\s\\S]*?\\n<helpfulness>\\n(.*?)\\n</helpfulness>'\n",
    "\n",
    "R1 = dict()\n",
    "\n",
    "all_member_urls = []\n",
    "R3_data_points = []\n",
    "\n",
    "for i in review_count:\n",
    "    tuple_ = re.findall(regex, i)\n",
    "    name = ((tuple_[0])[0])\n",
    "    score = ((tuple_[0])[1])\n",
    "    all_member_urls.append([name, score])\n",
    "    R3_data_points.append([name, score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = []\n",
    "for item in all_member_urls:\n",
    "    name = item[1]\n",
    "    value = name.replace(' of ', '/')\n",
    "    if value.endswith('/0'):\n",
    "        values.append(0)\n",
    "    else:\n",
    "        val = eval(value)\n",
    "        values.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(all_member_urls)):\n",
    "    all_member_urls[i][1] = values[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R1 = dict()\n",
    "\n",
    "for i in all_member_urls:\n",
    "    name = i[0]\n",
    "    score = i[1]\n",
    "    if name in R1:\n",
    "        R1[name].append(score)\n",
    "    else:\n",
    "        R1.setdefault(name, [])\n",
    "        R1[name].append(score)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R1_results = dict()\n",
    "\n",
    "for key in R1:\n",
    "    mean_val = mean(R1[key])\n",
    "    R1_results[key] = mean_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the R2 values dictionary to add to the data frame later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import stdev \n",
    "\n",
    "R2_results = dict()\n",
    "\n",
    "for key in R1:\n",
    "    if len(R1[key]) == 1:\n",
    "        mean_val = (R1[key])[0]\n",
    "        R2_results[key] = mean_val\n",
    "    else:\n",
    "        mean_val = stdev(R1[key])\n",
    "        R2_results[key] = mean_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = []\n",
    "\n",
    "regex = r'<memberUrl>\\n(.*?)\\n</memberUrl>'\n",
    "\n",
    "for i in review_count:\n",
    "    member_name = re.search(regex, i)\n",
    "    member_name = member_name[1]\n",
    "    R2.append(R2_results[member_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_R1 = dict()\n",
    "\n",
    "for i in R3_data_points:\n",
    "    name = i[0]\n",
    "    score = i[1]\n",
    "    if name in new_R1:\n",
    "        new_R1[name].append(score)\n",
    "    else:\n",
    "        new_R1.setdefault(name, [])\n",
    "        new_R1[name].append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the R3 values dictionary to add to the data frame later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R3_results = dict()\n",
    "\n",
    "percent = 0\n",
    "for key in new_R1:\n",
    "    list_of_vals = new_R1[key]\n",
    "    count = 0\n",
    "    total = 0\n",
    "    for i in list_of_vals:\n",
    "        numb = (i[-2:]).strip()\n",
    "        if int(numb) > 4:\n",
    "            count += 1\n",
    "        total += 1\n",
    "    percent = count/total\n",
    "    R3_results[key] = percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R3 = []\n",
    "\n",
    "regex = r'<memberUrl>\\n(.*?)\\n</memberUrl>'\n",
    "\n",
    "for i in review_count:\n",
    "    member_name = re.search(regex, i)\n",
    "    member_name = member_name[1]\n",
    "    R3.append(R3_results[member_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R1 = []\n",
    "\n",
    "regex = r'<memberUrl>\\n(.*?)\\n</memberUrl>'\n",
    "count = 0\n",
    "for i in review_count:\n",
    "    member_name = re.search(regex, i)\n",
    "    member_name = member_name[1]\n",
    "    R1.append(R1_results[member_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Features\n",
    "\n",
    "\n",
    "- SL1: The number of reviews authored by the user.\n",
    "- SL2: The mean number of reviews authored by all users.\n",
    "- SL3: The standard deviation of the number of reviews authored by all users.\n",
    "- SL4: The number of reviews submitted for the hotel.\n",
    "- SL5: The mean number of reviews submitted for all hotels.\n",
    "- SL6: The standard deviation of the number of reviews submitted for all hotels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below this code extracts the Socila features subset from the dataset and stores these into individual lists \n",
    "to then add to the data frame later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SL1_ = dict()\n",
    "\n",
    "for i in new_R1:\n",
    "    SL1_[i] = len(new_R1[i])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SL1 = []\n",
    "\n",
    "regex = r'<memberUrl>\\n(.*?)\\n</memberUrl>'\n",
    "\n",
    "for i in review_count:\n",
    "    member_name = re.search(regex, i)\n",
    "    member_name = member_name[1]\n",
    "    SL1.append(SL1_[member_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SL2 = 0\n",
    "total = []\n",
    "\n",
    "for key in SL1:\n",
    "    total.append(SL1[key])\n",
    "\n",
    "SL2 = mean(total)\n",
    "SL3 = stdev(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = r'<hotelUrl>\\n(.*?)\\n</hotelUrl>'\n",
    "\n",
    "SL4_ = dict()\n",
    "\n",
    "for i in review_count:\n",
    "    hotel_name = re.search(regex, i)\n",
    "    hotel_name = hotel_name[1]\n",
    "    if hotel_name in SL4_:\n",
    "        count = SL4_[hotel_name]\n",
    "        SL4_[hotel_name] = (count+1)\n",
    "    else:\n",
    "        SL4_[hotel_name] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SL4 = []\n",
    "\n",
    "regex = r'<hotelUrl>\\n(.*?)\\n</hotelUrl>'\n",
    "\n",
    "for i in review_count:\n",
    "    member_name = re.search(regex, i)\n",
    "    member_name = member_name[1]\n",
    "    SL4.append(SL4_[member_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SL5 = 0\n",
    "SL6 = 0\n",
    "\n",
    "total = []\n",
    "\n",
    "for key in SL4:\n",
    "    total.append(SL4[key])\n",
    "SL5 = mean(total)\n",
    "SL6 = stdev(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Features\n",
    "\n",
    "- ST1: The score assigned by the user to the hotel.\n",
    "- ST2: The number of (optional) sub-scores assigned by the user.\n",
    "- ST3: The mean sub-score assigned by the user.\n",
    "- ST4: The standard deviation of the sub-scores assigned by the user.\n",
    "- ST5: The mean score over all reviews authored by the user.\n",
    "- ST6: The standard deviation of the scores over all reviews authored by the user.\n",
    "- ST7: The mean score assigned by all users to the hotel.\n",
    "- ST8: The standard deviation of scores assigned by all users to the hotel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below this code extracts the Sentiment features subset from the dataset and stores these into individual lists to then add to the data frame later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "\n",
    "regex = r'<rating>\\n([\\s\\S]*?)\\n</rating>'\n",
    "\n",
    "ratings = []\n",
    "\n",
    "for i in review_count:\n",
    "    rating = re.findall(regex, i)\n",
    "    ratings.append(int(rating[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_regex = r'<value>\\n(.*?)\\n</value>'\n",
    "rooms_regex = r'<rooms>\\n(.*?)\\n</rooms>'\n",
    "location_regex = r'</rooms>\\n<location>\\n(.*?)\\n</location>'\n",
    "cleanliness_regex = r'<cleanliness>\\n(.*?)\\n</cleanliness>'\n",
    "frondesk_regex = r'<checkInFrontDesk>\\n(.*?)\\n</checkInFrontDesk>'\n",
    "service_regex = r'<service>\\n(.*?)\\n</service>'\n",
    "bizness_regex = r'<businessService>\\n(.*?)\\n</businessService>'\n",
    "\n",
    "ST2 = []\n",
    "\n",
    "for i in review_count:\n",
    "    count = 0\n",
    "    if re.search(value_regex, i):\n",
    "        val = re.search(value_regex, i)[1]\n",
    "        if val != 'null':\n",
    "            count+=1\n",
    "    if re.search(rooms_regex, i):\n",
    "        rooms = re.search(rooms_regex, i)[1]\n",
    "        if rooms != 'null':\n",
    "            count+=1\n",
    "    if re.search(location_regex, i):\n",
    "        location = re.search(location_regex, i)[1]\n",
    "        if location != 'null':\n",
    "            count+=1\n",
    "    if re.search(cleanliness_regex, i):\n",
    "        clean = re.search(cleanliness_regex, i)[1]\n",
    "        if clean != 'null':\n",
    "            count+=1\n",
    "    if re.search(frondesk_regex, i):\n",
    "        frontdesk = re.search(frondesk_regex, i)[1]\n",
    "        if frontdesk != 'null':\n",
    "            count+=1\n",
    "    if re.search(bizness_regex, i):\n",
    "        bizness = re.search(bizness_regex, i)[1]\n",
    "        if bizness != 'null':\n",
    "            count+=1\n",
    "    if re.search(service_regex, i):\n",
    "        service = re.search(service_regex, i)[1]\n",
    "        if service != 'null':\n",
    "            count+=1\n",
    "    \n",
    "    ST2.append(count)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ST3 = mean(ST2)\n",
    "ST4 = stdev(ST2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = r'<memberUrl>\\n(.*?)\\n</memberUrl>\\n[\\s\\S]*?\\n<rating>\\n(.*?)\\n</rating>'\n",
    "\n",
    "member_score = dict()\n",
    "member_stdev = dict()\n",
    "\n",
    "for i in review_count:\n",
    "    regexp = re.findall(regex, i)\n",
    "    name = regexp[0][0]\n",
    "    rating = int(regexp[0][1])\n",
    "    if name in member_score:\n",
    "        member_score[name].append(rating)\n",
    "    else:\n",
    "        member_score[name] = [rating]\n",
    "\n",
    "member_stdev = member_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "member_mean = dict()\n",
    "\n",
    "for i in member_score:\n",
    "    member_mean[i] = mean(member_score[i])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in member_score:\n",
    "    if len(member_score[i]) < 2:\n",
    "        member_stdev[i] = member_score[i]\n",
    "    else:\n",
    "        member_stdev[i] = stdev(member_score[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ST5 = []\n",
    "\n",
    "regex = r'<memberUrl>\\n(.*?)\\n</memberUrl>'\n",
    "count = 0\n",
    "for i in review_count:\n",
    "    member_name = re.search(regex, i)\n",
    "    member_name = member_name[1]\n",
    "    ST5.append(member_mean[member_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ST6 = []\n",
    "\n",
    "regex = r'<memberUrl>\\n(.*?)\\n</memberUrl>'\n",
    "count = 0\n",
    "for i in review_count:\n",
    "    member_name = re.search(regex, i)\n",
    "    member_name = member_name[1]\n",
    "    val = member_stdev[member_name]\n",
    "    if isinstance(val, list):\n",
    "        ST6.append(val[0])\n",
    "    else:\n",
    "        ST6.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ST7 = []\n",
    "\n",
    "regex = r'<hotelRatingHistogram>\\n(.*?)\\n</hotelRatingHistogram>'\n",
    "count_regex = r'<numberHotelRatings>\\n(.*?)\\n</numberHotelRatings>'\n",
    "standard_dev = []\n",
    "\n",
    "for i in review_count:\n",
    "    nums = re.search(regex, i)[1]\n",
    "    count = re.search(count_regex, i)[1]\n",
    "    list_nums = nums.split(',')\n",
    "    total = (int(list_nums[0])*5)+(int(list_nums[1])*4)+(int(list_nums[2])*3)+(int(list_nums[3])*2)+(int(list_nums[4])*1)\n",
    "    mean = total/int(count)\n",
    "    ST7.append(mean)\n",
    "    all_nums = []\n",
    "    for five in range(int(list_nums[0])):\n",
    "        all_nums.append(5)\n",
    "    for four in range(int(list_nums[1])):\n",
    "        all_nums.append(4)\n",
    "    for three in range(int(list_nums[2])):\n",
    "        all_nums.append(3)\n",
    "    for two in range(int(list_nums[3])):\n",
    "        all_nums.append(2)\n",
    "    for one in range(int(list_nums[4])):\n",
    "        all_nums.append(1)\n",
    "    if len(all_nums) < 2:\n",
    "        standard_dev.append(all_nums)\n",
    "    else:\n",
    "        standard_dev.append(stdev(all_nums))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ST8 = []\n",
    "for i in standard_dev:\n",
    "    if isinstance(i, list):\n",
    "        ST8.append(i[0])\n",
    "    else:\n",
    "        ST8.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content Features\n",
    "\n",
    "- C1: The number of terms in the review text.\n",
    "- C2: The ratio of uppercase and lowercase characters to other characters in the review text.\n",
    "- C3: The ratio of uppercase to lowercase characters in the review text.\n",
    "- C4: Review completeness (a) – an integer in the range [0,2] which captures whether the user has completed one, both or none of the optional liked and disliked parts of the review (see Section 2.1).\n",
    "- C5: Review completeness (b) – the number of optional personal and purpose of visit details that are provided by the user in the review (see Section 2.1).\n",
    "- C6: Review completeness (c) – the number of optional review-template questions that are answered in the review (see Section 2.1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below this code extracts the Content features subset from the dataset and stores these into individual lists to then add to the data frame later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = []\n",
    "C1 = []\n",
    "\n",
    "text_regex = r'<reviewText>\\n(.*?)</p>\\n</reviewText>'\n",
    "\n",
    "for i in review_count:\n",
    "    text = re.search(text_regex, i)\n",
    "    if text is not None:\n",
    "        new_text = re.sub('<br/>', '', text[1])\n",
    "        new_text = re.sub('&amp;', ' ', new_text)\n",
    "        new_text = re.sub('&quot;', ' ', new_text)\n",
    "\n",
    "        text_list = new_text.split(' ')\n",
    "        C1.append(len(text_list)) \n",
    "    else:\n",
    "        C1.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "C2 = []\n",
    "count = lambda l1,l2: sum([1 for x in l1 if x in l2])\n",
    "\n",
    "for i in review_count:\n",
    "    text = re.search(text_regex, i)\n",
    "    if text is not None:\n",
    "        text = text[1]\n",
    "        lower_count = sum(1 for c in text if c.islower())\n",
    "        upper_count = sum(1 for c in text if c.isupper())\n",
    "        punc_count = count(text,set(string.punctuation)) \n",
    "        ans = punc_count/(lower_count+upper_count)\n",
    "        C2.append(ans)\n",
    "    else:\n",
    "        C2.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C3 = []\n",
    "count = lambda l1,l2: sum([1 for x in l1 if x in l2])\n",
    "\n",
    "for i in review_count:\n",
    "    text = re.search(text_regex, i)\n",
    "    if text is not None:\n",
    "        text = text[1]\n",
    "        lower_count = sum(1 for c in text if c.islower())\n",
    "        upper_count = sum(1 for c in text if c.isupper())\n",
    "        ans = (upper_count/lower_count)\n",
    "        C3.append(ans)\n",
    "    else:\n",
    "        C3.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C4 = []\n",
    "\n",
    "liked_regex = r'<liked>\\n(.*?)\\n</liked>'\n",
    "disliked_regex = r'<disliked>\\n(.*?)\\n</disliked>'\n",
    "\n",
    "for i in review_count:\n",
    "    count = 0\n",
    "    liked = re.search(liked_regex, i)\n",
    "    disliked = re.search(disliked_regex, i)\n",
    "    if str(liked[1]) != 'null':\n",
    "        count+=1\n",
    "    if str(disliked[1]) != 'null':\n",
    "        count +=1\n",
    "    C4.append(count) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_stay = r'<dateOfStay>\\n(.*?)\\n</dateOfStay>'\n",
    "regex_reason= r'<visitWasFor>\\n(.*?)\\n</visitWasFor>'\n",
    "regex_age= r'<ageRange>\\n(.*)\\n</ageRange>'\n",
    "regex_since= r'<memberSince>\\n(.*?)\\n</memberSince>'\n",
    "regex_group = r'<travelingGroup>\\n(.*?)\\n</travelingGroup>'\n",
    "\n",
    "C5 = []\n",
    "\n",
    "for i in review_count:\n",
    "    stay = (re.search(regex_stay, i))[1]\n",
    "    reason = (re.search(regex_reason, i))[1]\n",
    "    age = (re.search(regex_age, i))[1]\n",
    "    since = (re.search(regex_since, i))[1]\n",
    "    group = (re.search(regex_group, i))[1]\n",
    "    count = 0\n",
    "    if stay != 'null':\n",
    "        count+=1\n",
    "    if reason != 'null':\n",
    "        count+=1\n",
    "    if age != 'null':\n",
    "        count+=1\n",
    "    if since != 'null':\n",
    "        count+=1\n",
    "    if group != 'null':\n",
    "        count+=1\n",
    "    C5.append(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_qs = r'<numQuestions>\\n(.*?)\\n</numQuestions>'\n",
    "\n",
    "C6 = []\n",
    "\n",
    "for i in review_count:\n",
    "    qs = (re.search(regex_qs, i))[1]\n",
    "    \n",
    "    C6.append(int(qs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2\n",
    "\n",
    "- Creating dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create the data frame using all the feature subsets we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(C1, C2, C3, C4, C5, C6)), columns =['C1', 'C2', 'C3', 'C4', 'C5', 'C6'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(R1, R2, R3, C1, C2, C3, C4, C5, C6, SL1, SL4, ratings, ST2, ST7, ST8, hotel_id, reviewer_id)),  \n",
    "     columns =['R1', 'R2', 'R3','C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'SL1', 'SL4', 'ST1', 'ST2', 'ST7', 'ST8', 'hotel_id', 'reviewer_id'])\n",
    "\n",
    "df['SL2'] = SL2\n",
    "df['SL3'] = SL3\n",
    "df['ST3'] = ST3\n",
    "df['ST4'] = ST4\n",
    "df['ST5'] = ST5\n",
    "df['ST6'] = ST6\n",
    "df['SL5'] = SL5\n",
    "df['SL6'] = SL6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hotel_id'] = hotel_id\n",
    "df['reviewer_id'] = reviewer_id\n",
    "df = df.set_index(['hotel_id', 'reviewer_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set the helpfulness score of the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "for i in helpfulness_scores:\n",
    "    if i < 0.75:\n",
    "        helpfulness_scores[count] = 0\n",
    "    else:\n",
    "        helpfulness_scores[count] = 1\n",
    "    count += 1\n",
    "\n",
    "df['helpful_of_not'] = helpfulness_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then take the balanced sample of the data frame. Then we write this sample to a .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modified = df.copy()\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_majority = df_modified[df_modified.helpful_of_not==1]\n",
    "df_minority = df_modified[df_modified.helpful_of_not==0]\n",
    " \n",
    "# Downsample majority class\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                 replace=False,    # sample without replacement\n",
    "                                 n_samples=12519)#,     # to match minority class\n",
    "#random_state=4) \n",
    " \n",
    "# Combine minority class with downsampled majority class\n",
    "df_modified = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_majority = df_modified[df_modified.helpful_of_not==0]\n",
    "df_minority = df_modified[df_modified.helpful_of_not==1]\n",
    " \n",
    "# Downsample majority class\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                 replace=False,    # sample without replacement\n",
    "                                 n_samples=12519)#,     # to match minority class\n",
    "#random_state=4) \n",
    " \n",
    "# Combine minority class with downsampled majority class\n",
    "df_modified = pd.concat([df_majority_downsampled, df_minority])\n",
    " \n",
    "# Display new class counts\n",
    "df_modified.helpful_of_not.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if user_decision == 1:\n",
    "    df.to_csv('chicago_data_frame.csv')\n",
    "else:\n",
    "    df.to_csv('las-vegas_data_frame.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3\n",
    "\n",
    "- Here we get the new feature subset. Add this to the data frame and then write this data frame to a .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllable_count(word):\n",
    "    word = word.lower()\n",
    "    count = 0\n",
    "    vowels = \"aeiouy\"\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "    if word.endswith(\"e\"):\n",
    "        count -= 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_complex_words = []\n",
    "\n",
    "text_regex = r'<reviewText>\\n(.*?)</p>\\n</reviewText>'\n",
    "\n",
    "for i in review_count:\n",
    "    text = re.search(text_regex, i)\n",
    "    if text is not None:\n",
    "        new_text = re.sub('<br/>', '', text[1])\n",
    "        new_text = re.sub('&amp;', ' ', new_text)\n",
    "        new_text = re.sub('&quot;', ' ', new_text)\n",
    "        new_text = re.sub('<p id=\\\"review_[\\d]*?\\\">', '',new_text)\n",
    "\n",
    "        text_list = re.findall(r\"[\\w']+\", new_text)\n",
    "        complex_word = 0\n",
    "        count = 0\n",
    "        for i in text_list:\n",
    "            if len(i) > 0:\n",
    "                count = syllable_count(i)\n",
    "                if count > 2:\n",
    "                    complex_word +=1\n",
    "            else:\n",
    "                complex_word += 0\n",
    "        number_of_complex_words.append(complex_word)\n",
    "    else:\n",
    "        number_of_complex_words.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_complex_words'] = number_of_complex_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_syllables_per_word = []\n",
    "\n",
    "text_regex = r'<reviewText>\\n(.*?)</p>\\n</reviewText>'\n",
    "\n",
    "for i in review_count:\n",
    "    text = re.search(text_regex, i)\n",
    "    if text is not None:\n",
    "        new_text = re.sub('<br/>', '', text[1])\n",
    "        new_text = re.sub('&amp;', ' ', new_text)\n",
    "        new_text = re.sub('&quot;', ' ', new_text)\n",
    "        new_text = re.sub('<p id=\\\"review_[\\d]*?\\\">', '',new_text)\n",
    "\n",
    "        text_list = re.findall(r\"[\\w']+\", new_text)\n",
    "        count = []\n",
    "        for i in text_list:\n",
    "            if len(i) > 0:\n",
    "                count.append(syllable_count(i))\n",
    "            else:\n",
    "                count.append(0)\n",
    "        if len(count) > 1:\n",
    "            avg_syllables_per_word.append(statistics.mean(count))\n",
    "        else:\n",
    "            avg_syllables_per_word.append(count)\n",
    "    else:\n",
    "        avg_syllables_per_word.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['avg_syllables_per_review'] = avg_syllables_per_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_words_per_sen = []\n",
    "\n",
    "text_regex = r'<reviewText>\\n(.*?)</p>\\n</reviewText>'\n",
    "\n",
    "for i in review_count:\n",
    "    text = re.search(text_regex, i)\n",
    "    if text is not None:\n",
    "        new_text = re.sub('<br/>', '', text[1])\n",
    "        new_text = re.sub('&amp;', ' ', new_text)\n",
    "        new_text = re.sub('&quot;', ' ', new_text)\n",
    "        new_text = re.sub('<p id=\\\"review_[\\d]*?\\\">', '',new_text)\n",
    "\n",
    "        text_list = re.split(r'[.?!]\\s*', new_text)\n",
    "        count = []\n",
    "        for i in text_list:\n",
    "            i = i.split(' ')\n",
    "            if len(i) > 0:\n",
    "                count.append(len(i))\n",
    "        if len(count) > 1:\n",
    "            avg_words_per_sen.append(statistics.mean(count))\n",
    "        else:\n",
    "            avg_words_per_sen.append(0)\n",
    "    else:\n",
    "        avg_words_per_sen.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['avg_words_per_sen'] = avg_words_per_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gunning_fog_score = []\n",
    "\n",
    "text_regex = r'<reviewText>\\n(.*?)</p>\\n</reviewText>'\n",
    "\n",
    "for i in review_count:\n",
    "    text = re.search(text_regex, i)\n",
    "    if text is not None:\n",
    "        new_text = re.sub('<br/>', '', text[1])\n",
    "        new_text = re.sub('&amp;', ' ', new_text)\n",
    "        new_text = re.sub('&quot;', ' ', new_text)\n",
    "        new_text = re.sub('<p id=\\\"review_[\\d]*?\\\">', '',new_text)\n",
    "        gunning_fog_score.append(textstat.gunning_fog(new_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gunning_fog_score'] = gunning_fog_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flesch_reading_ease_score = []\n",
    "\n",
    "text_regex = r'<reviewText>\\n(.*?)</p>\\n</reviewText>'\n",
    "\n",
    "for i in review_count:\n",
    "    text = re.search(text_regex, i)\n",
    "    if text is not None:\n",
    "        new_text = re.sub('<br/>', '', text[1])\n",
    "        new_text = re.sub('&amp;', ' ', new_text)\n",
    "        new_text = re.sub('&quot;', ' ', new_text)\n",
    "        new_text = re.sub('<p id=\\\"review_[\\d]*?\\\">', '',new_text)\n",
    "        flesch_reading_ease_score.append(textstat.flesch_reading_ease(new_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['flesch_reading_ease_score'] = flesch_reading_ease_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flesch_kincaid_grade_score = []\n",
    "\n",
    "text_regex = r'<reviewText>\\n(.*?)</p>\\n</reviewText>'\n",
    "\n",
    "for i in review_count:\n",
    "    text = re.search(text_regex, i)\n",
    "    if text is not None:\n",
    "        new_text = re.sub('<br/>', '', text[1])\n",
    "        new_text = re.sub('&amp;', ' ', new_text)\n",
    "        new_text = re.sub('&quot;', ' ', new_text)\n",
    "        new_text = re.sub('<p id=\\\"review_[\\d]*?\\\">', '',new_text)\n",
    "        flesch_kincaid_grade_score.append(textstat.flesch_kincaid_grade(new_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['flesch_kincaid_grade_score'] = flesch_kincaid_grade_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smog_index_score = []\n",
    "\n",
    "text_regex = r'<reviewText>\\n(.*?)</p>\\n</reviewText>'\n",
    "\n",
    "for i in review_count:\n",
    "    text = re.search(text_regex, i)\n",
    "    if text is not None:\n",
    "        new_text = re.sub('<br/>', '', text[1])\n",
    "        new_text = re.sub('&amp;', ' ', new_text)\n",
    "        new_text = re.sub('&quot;', ' ', new_text)\n",
    "        new_text = re.sub('<p id=\\\"review_[\\d]*?\\\">', '',new_text)\n",
    "        smog_index_score.append(textstat.smog_index(new_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['smog_index_score'] = smog_index_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index(['hotel_id', 'reviewer_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate majority and minority classes\n",
    "df_majority = df[df.helpful_of_not==1]\n",
    "df_minority = df[df.helpful_of_not==0]\n",
    " \n",
    "# Downsample majority class\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                 replace=False,    # sample without replacement\n",
    "                                 n_samples=12519)#,     # to match minority class\n",
    "#random_state=4) \n",
    " \n",
    "# Combine minority class with downsampled majority class\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    " \n",
    "# Display new class counts\n",
    "df_downsampled.helpful_of_not.value_counts()\n",
    "\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_majority = df_downsampled[df_downsampled.helpful_of_not==0]\n",
    "df_minority = df_downsampled[df_downsampled.helpful_of_not==1]\n",
    " \n",
    "# Downsample majority class\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                 replace=False,    # sample without replacement\n",
    "                                 n_samples=12519)#,     # to match minority class\n",
    "#random_state=4) \n",
    " \n",
    "# Combine minority class with downsampled majority class\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    " \n",
    "# Display new class counts\n",
    "df_downsampled.helpful_of_not.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if user_decision == 1:\n",
    "    df_downsampled.to_csv('chicago_data_frame_with_more_rows.csv')\n",
    "else:\n",
    "    df_downsampled.to_csv('las-vegas_data_frame_with_more_rows.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
